import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
import os

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'SimHei', 'Arial Unicode MS', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False 


from lib.layerwrapper import WrappedGPT


# --- é…ç½®å‚æ•° ---
# æ¨¡æ‹Ÿ Llama2-7B MLP å±‚ (å¦‚ gate_proj/up_proj) çš„ç»´åº¦
# Llama2-7B hidden_size = 4096, intermediate_size = 11008
INPUT_DIM = 4096    # çº¿æ€§å±‚è¾“å…¥ç‰¹å¾ç»´åº¦ (æ¨¡æ‹Ÿ hidden_size)
OUTPUT_DIM = 11008   # çº¿æ€§å±‚è¾“å‡ºç‰¹å¾ç»´åº¦ (æ¨¡æ‹Ÿ intermediate_size)

NUM_SAMPLES = 2000 # ç”¨äºç”Ÿæˆæ¿€æ´»æ•°æ®çš„æ ·æœ¬æ•°é‡
# TRAIN_EPOCHS = 1000 # ä¸å†éœ€è¦è®­ç»ƒï¼Œæ³¨é‡Šæ‰



SEED = 42
torch.manual_seed(SEED)
np.random.seed(SEED)

# å°†è®¡ç®—ç§»åŠ¨åˆ° GPU (å¦‚æœå¯ç”¨)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# --- è¾…åŠ©å‡½æ•°ï¼šè®¡ç®—ç¨€ç–åº¦ ---
def calculate_sparsity(weight_matrix):
    """è®¡ç®—çŸ©é˜µçš„ç¨€ç–åº¦ (é›¶å…ƒç´ çš„æ¯”ä¾‹)"""
    total_elements = weight_matrix.numel()
    zero_elements = (weight_matrix == 0).sum().item()
    sparsity = zero_elements / total_elements
    return sparsity

# --- 2:4 ç»“æ„åŒ–å‰ªæå‡½æ•° ---
def prune_to_2_4(matrix, scaler_row):
    """
    å¯¹è¾“å…¥çŸ©é˜µæŒ‰è¡Œåš2:4ç»“æ„åŒ–å‰ªæï¼šæ¯4ä¸ªå…ƒç´ ä¿ç•™2ä¸ªæœ€å¤§å¹…åº¦ï¼Œå…¶ä½™ç½®é›¶ã€‚
    ä¿®æ”¹ä¸ºWandaåŸç‰ˆå¤„ç†æ–¹å¼ï¼šå¿½ç•¥æœ«å°¾ä¸è¶³4åˆ—çš„éƒ¨åˆ†ã€‚
    """
    # è®¡ç®—æƒé‡é‡è¦æ€§ï¼Œä½¿ç”¨WandaåŸç‰ˆçš„åº¦é‡æ–¹æ³•
    # Wanda çš„åº¦é‡æ ‡å‡†: abs(W) * sqrt(scaler_row)
    W_metric = torch.abs(matrix) * torch.sqrt(scaler_row.reshape((1, -1)))
    
    # åˆ›å»ºå‰ªææ©ç 
    W_mask = torch.zeros_like(W_metric, dtype=torch.bool)
    
    # 2:4ç»“æ„åŒ–å‰ªæï¼šæ¯4åˆ—ä¸­ä¿ç•™2ä¸ªæœ€é‡è¦çš„ï¼Œå‰ªæ‰2ä¸ªæœ€ä¸é‡è¦çš„
    prune_n, prune_m = 2, 4  # æ¯4ä¸ªå…ƒç´ ä¸­å‰ªæ‰2ä¸ªï¼ˆä¿ç•™2ä¸ªï¼‰
    
    # WandaåŸç‰ˆé€»è¾‘ï¼šåªå¤„ç†å®Œæ•´çš„4åˆ—å—
    for ii in range(0, W_metric.shape[1] - W_metric.shape[1] % prune_m, prune_m):
        # å®Œæ•´çš„4åˆ—å—ï¼Œæ­£å¸¸2:4å‰ªæ
        tmp = W_metric[:, ii:ii+prune_m].float()
        # topkè¿”å›æœ€å°çš„prune_nä¸ªå…ƒç´ çš„ç´¢å¼•ï¼Œlargest=Falseè¡¨ç¤ºå–æœ€å°çš„
        _, indices = torch.topk(tmp, prune_n, dim=1, largest=False)
        # åœ¨æ©ç ä¸­æ ‡è®°è¦å‰ªæçš„ä½ç½®
        W_mask.scatter_(1, ii + indices, True)

    # åº”ç”¨æ©ç è¿›è¡Œå‰ªæ
    result = matrix.clone()
    result[W_mask] = 0
    
    return result


# --- Wandaéç»“æ„åŒ–å‰ªæå‡½æ•° ---
def wanda_unstructured_prune(matrix, scaler_row, sparsity_ratio=0.5):
    """
    ä½¿ç”¨Wandaé‡è¦æ€§åº¦é‡è¿›è¡Œéç»“æ„åŒ–å‰ªæ
    Args:
        matrix: è¾“å…¥æƒé‡çŸ©é˜µ
        scaler_row: æ¿€æ´»ç»Ÿè®¡ä¿¡æ¯
        sparsity_ratio: ç¨€ç–ç‡ (0.5è¡¨ç¤º50%ç¨€ç–)
    """
    print(f"  åº”ç”¨Wandaéç»“æ„åŒ–å‰ªæï¼Œç¨€ç–ç‡: {sparsity_ratio:.1%}")
    
    # è®¡ç®—Wandaé‡è¦æ€§åº¦é‡
    W_metric = torch.abs(matrix) * torch.sqrt(scaler_row.reshape((1, -1)))
    
    # è®¡ç®—éœ€è¦å‰ªæçš„å…ƒç´ æ•°é‡
    total_elements = matrix.numel()
    num_prune = int(total_elements * sparsity_ratio)
    
    # æ‰¾åˆ°é‡è¦æ€§æœ€å°çš„å…ƒç´ è¿›è¡Œå‰ªæ
    threshold = torch.kthvalue(W_metric.flatten(), num_prune).values
    mask = W_metric <= threshold
    
    # åº”ç”¨å‰ªææ©ç 
    result = matrix.clone()
    result[mask] = 0
    
    # éªŒè¯ç¨€ç–ç‡
    actual_sparsity = (result == 0).float().mean().item()
    print(f"  å®é™…ç¨€ç–ç‡: {actual_sparsity:.1%}")
    
    return result



# --- 2. åº”ç”¨Wandaéç»“æ„åŒ–å‰ªæå¾—åˆ°W_unstructured ---
def apply_wanda_unstructured_pruning(W_dense, scaler_row, sparsity_ratio=0.5):
    print(f"\n--- 2. åº”ç”¨Wandaéç»“æ„åŒ–å‰ªæ (ç¨€ç–ç‡: {sparsity_ratio:.1%}) ---")
    
    W_unstructured = wanda_unstructured_prune(W_dense, scaler_row, sparsity_ratio)
    
    # éªŒè¯ç¨€ç–æ€§
    actual_sparsity = calculate_sparsity(W_unstructured)
    print(f"  W_unstructured å½¢çŠ¶: {W_unstructured.shape}, ç¨€ç–ç‡: {actual_sparsity:.1%}")
    print(f"  W_unstructured norm: {torch.norm(W_unstructured):.4f}")
    
    return W_unstructured

# --- 2:4 åŸºçº¿å‰ªææ–¹æ³• ---
def prune_2_4_baseline(weight_matrix, scaler_row):
    """
    ç›´æ¥å¯¹æƒé‡çŸ©é˜µè¿›è¡Œ2:4ç»“æ„åŒ–å‰ªæä½œä¸ºåŸºçº¿
    """
    print(f"\n--- 2:4 åŸºçº¿å‰ªæ ---")
    print(f"  æ­£åœ¨å¤„ç† {weight_matrix.shape} çš„æƒé‡çŸ©é˜µ...")
    print(f"  ä½¿ç”¨Wandaé‡è¦æ€§åº¦é‡ (æƒé‡ * sqrt(scaler_row))")
    result = prune_to_2_4(weight_matrix, scaler_row)
    print(f"  2:4 åŸºçº¿å‰ªæå®Œæˆ")
    return result




def check_2_4_sparsity(matrix):
    """
    æ£€æŸ¥çŸ©é˜µæ˜¯å¦ç¬¦åˆ2:4ç¨€ç–æ¨¡å¼
    åŸºäºWandaåŸå§‹é€»è¾‘å®ç°
    """
    rows, cols = matrix.shape
    
    # æ£€æŸ¥æ¯4åˆ—ä¸ºä¸€ç»„ï¼Œæ¯ç»„ä¸­æ˜¯å¦æœ€å¤šæœ‰2ä¸ªéé›¶å…ƒç´ 
    prune_m = 4
    tolerance = 1e-10
    
    for ii in range(0, cols, prune_m):
        # å–å‡ºå½“å‰çš„4åˆ—å—ï¼ˆå¦‚æœä¸è¶³4åˆ—åˆ™å–å‰©ä½™çš„ï¼‰
        end_col = min(ii + prune_m, cols)
        if end_col - ii < prune_m:
            # å¦‚æœæœ€åä¸€ç»„ä¸è¶³4åˆ—ï¼Œæˆ‘ä»¬å¯ä»¥æ”¾å®½æ£€æŸ¥
            continue
            
        block = matrix[:, ii:end_col]  # [rows, 4]
        
        # è®¡ç®—æ¯è¡Œä¸­éé›¶å…ƒç´ çš„æ•°é‡
        non_zero_count = (torch.abs(block) > tolerance).sum(dim=1)
        
        # æ£€æŸ¥æ¯è¡Œæ˜¯å¦æœ€å¤šæœ‰2ä¸ªéé›¶å…ƒç´ 
        if torch.any(non_zero_count > 2):
            return False
    
    return True



# --- è¾…åŠ©å‡½æ•°ï¼šè®¡ç®— Î”B çš„ç§©å’Œæ ¸èŒƒæ•° ---
def calculate_delta_metrics(W_unstructured, W_main_pruned, svd_tolerance=1e-10):
    """
    è®¡ç®— Î”B = W_unstructured - W_main_pruned çš„æ ¸èŒƒæ•°å’Œæœ‰æ•ˆç§©ã€‚
    æœ‰æ•ˆç§©é€šè¿‡è¿‡æ»¤å°äºsvd_toleranceçš„å¥‡å¼‚å€¼æ¥è®¡ç®—ã€‚
    """
    delta_B = W_unstructured - W_main_pruned
    
    nuclear_norm_delta_B = torch.linalg.matrix_norm(delta_B, ord='nuc').item()

    rank_delta_B = 0
    try:
        if torch.norm(delta_B) > 1e-12: # é¿å…å¯¹å‡ ä¹ä¸ºé›¶çš„çŸ©é˜µè¿›è¡ŒSVD
            U, S_vals, Vh = torch.linalg.svd(delta_B, full_matrices=False)
            effective_singular_values = S_vals[S_vals > svd_tolerance]
            rank_delta_B = len(effective_singular_values)
        else:
            rank_delta_B = 0
    except RuntimeError:
        rank_delta_B = 0 # SVDè®¡ç®—å¤±è´¥æ—¶ç§©ä¸º0
    
    return rank_delta_B, nuclear_norm_delta_B



# --- åŠŸèƒ½æŸå¤±è®¡ç®—å‡½æ•° ---
def functional_loss(W1, W2, X):
    """
    è®¡ç®—L2 ErroræŸå¤±: ||W1 @ X.T - W2 @ X.T||_F
    """
    return torch.norm((W1 - W2) @ X.T, p='fro').item()


# ä½ç§©è¿‘ä¼¼å‡½æ•°
def low_rank_approximation(matrix, k):
    """
    å¯¹è¾“å…¥çŸ©é˜µåšä½ç§©è¿‘ä¼¼ï¼ˆæˆªæ–­SVDï¼‰ï¼Œåªä¿ç•™å‰kä¸ªå¥‡å¼‚å€¼åˆ†é‡
    Args:
        matrix: è¾“å…¥çŸ©é˜µ (torch.Tensor)
        k: ä¿ç•™çš„ç§©
    Returns:
        ä½ç§©è¿‘ä¼¼çŸ©é˜µ (torch.Tensor)
    """
    original_device = matrix.device
    try:
        # å°è¯•åœ¨åŸè®¾å¤‡ä¸Šè¿›è¡ŒSVD
        U, S, Vh = torch.linalg.svd(matrix, full_matrices=False)
    except RuntimeError as e:
        if "out of memory" in str(e):
            print(f"    GPUå†…å­˜ä¸è¶³ï¼Œå°†SVDè®¡ç®—ç§»åˆ°CPU: {e}")
            # ç§»åˆ°CPUè¿›è¡Œè®¡ç®—
            matrix_cpu = matrix.cpu()
            U, S, Vh = torch.linalg.svd(matrix_cpu, full_matrices=False)
            # å°†ç»“æœç§»å›åŸè®¾å¤‡
            U = U.to(original_device)
            S = S.to(original_device)
            Vh = Vh.to(original_device)
        else:
            raise e
    
    S_trunc = torch.zeros_like(S)
    S_trunc[:k] = S[:k]
    approx = (U * S_trunc.unsqueeze(0)) @ Vh
    return approx


# --- 1. ç”Ÿæˆéšæœºç¨ å¯†çŸ©é˜µå’Œæ¨¡æ‹Ÿæ¿€æ´»ç»Ÿè®¡ ---
def generate_dense_matrix_and_activations(input_dim, output_dim, num_samples):
    print("\n--- 1. ç”Ÿæˆéšæœºç¨ å¯†æƒé‡çŸ©é˜µå’Œæ¨¡æ‹Ÿæ¿€æ´»ç»Ÿè®¡ ---")
    
    # ç”Ÿæˆéšæœºç¨ å¯†æƒé‡çŸ©é˜µ (out_features, in_features)
    #W_dense = torch.randn(output_dim, input_dim, dtype=torch.float32, device=device)
    # ä½¿ç”¨Xavieråˆå§‹åŒ–
    # ä½¿ç”¨kaimingåˆå§‹åŒ–
    #torch.nn.init.xavier_uniform_(W_dense)
    temp = torch.nn.Linear(input_dim, output_dim).to(device)
    W_dense = temp.weight.data
    print(f"  ç”Ÿæˆç¨ å¯†æƒé‡çŸ©é˜µ: {W_dense.shape}, (norm: {torch.norm(W_dense):.4f})")
    
    # ç”Ÿæˆæ¨¡æ‹Ÿçš„æ¿€æ´»æ•°æ®æ¥è®¡ç®—scaler_row
    print(f"  ç”Ÿæˆ {num_samples} ä¸ªæ ·æœ¬çš„æ¨¡æ‹Ÿæ¿€æ´»æ•°æ®...")
    X_activation = torch.randn(num_samples, input_dim, dtype=torch.float32, device=device)
    
    # åˆ›å»ºä¸´æ—¶çº¿æ€§å±‚æ¥ä½¿ç”¨WrappedGPTè®¡ç®—æ¿€æ´»ç»Ÿè®¡
    temp_model = nn.Linear(input_dim, output_dim, bias=False).to(device)
    temp_model.weight.data = W_dense.clone()
    
    print("  ä½¿ç”¨ WrappedGPT è®¡ç®— scaler_row (æ¿€æ´»ç»Ÿè®¡)...")
    wrapped_layer = WrappedGPT(temp_model)
    
    # è®¡ç®—æ¿€æ´»ç»Ÿè®¡ï¼Œç”¨äºç®—wandaé‡è¦æ€§æŒ‡æ ‡
    with torch.no_grad():
        batch_size = 32
        for i in range(0, num_samples, batch_size):
            batch_end = min(i + batch_size, num_samples)
            batch_input = X_activation[i:batch_end]
            batch_output = temp_model(batch_input)
            wrapped_layer.add_batch(batch_input, batch_output)
    
    scaler_row = wrapped_layer.scaler_row.detach().clone()
    print(f"  è®¡ç®—çš„ scaler_row å½¢çŠ¶: {scaler_row.shape}, (norm: {torch.norm(scaler_row):.4f})")
    
    return W_dense, scaler_row, X_activation






# --- åŸºäºæ ¸èŒƒæ•° subgradient çš„å•æ­¥ä¼˜åŒ– 2:4 å‰ªæ ---
def prune_2_4_with_nuclear_norm_importance(W_unstructured, W_dense):
    """
    åŸºäºæ ¸èŒƒæ•°subgradientçš„å•æ­¥ä¼˜åŒ–2:4å‰ªæã€‚
    è¯¥æ–¹æ³•æ—¨åœ¨æ‰¾åˆ°ä¸€ä¸ª2:4ç¨€ç–çŸ©é˜µ W_main, ä½¿å¾— ||W_unstructured - W_main||_* æœ€å°åŒ–ã€‚
    W_main æ˜¯é€šè¿‡å¯¹ W_dense è¿›è¡Œ2:4å‰ªæå¾—åˆ°çš„ã€‚

    æ­¥éª¤:
    1. è®¡ç®—å˜åŒ–çŸ©é˜µ Î”B = W_unstructured - W_denseã€‚
    2. å¯¹ Î”B è¿›è¡ŒSVD, å¾—åˆ°æ ¸èŒƒæ•°çš„subgradient G = U @ Vhã€‚
    3. è®¡ç®—é‡è¦æ€§åˆ†æ•°: importance = |Î”B * G|ã€‚
    4. ä½¿ç”¨æ­¤ importance åˆ†æ•°å¯¹ W_dense è¿›è¡Œ2:4å‰ªæï¼Œå¾—åˆ°æœ€ç»ˆçš„ W_mainã€‚

    Args:
        W_unstructured (torch.Tensor): 50%éç»“æ„åŒ–ç¨€ç–çš„ç›®æ ‡çŸ©é˜µã€‚
        W_dense (torch.Tensor): åŸå§‹çš„ç¨ å¯†çŸ©é˜µã€‚

    Returns:
        torch.Tensor: ç»è¿‡æ ¸èŒƒæ•°ä¼˜åŒ–å‰ªæåçš„2:4ç¨€ç–çŸ©é˜µã€‚
    """
    print(f"\n--- åŸºäºæ ¸èŒƒæ•° subgradient çš„å•æ­¥ä¼˜åŒ– 2:4 å‰ªæ ---")
    
    # 1. è®¡ç®—å˜åŒ–çŸ©é˜µ Î”B = W_unstructured - W_dense
    delta_B = W_unstructured - W_dense
    print(f"  è®¡ç®— Î”B (W_unstructured - W_dense), norm: {torch.norm(delta_B):.4f}")

    try:
        # 2. å¯¹ Î”B è¿›è¡ŒSVD, å¾—åˆ° subgradient G
        # ä½¿ç”¨ float32 ä»¥æé«˜ SVD çš„ç¨³å®šæ€§
        U, S, Vh = torch.linalg.svd(delta_B.to(torch.float32), full_matrices=False)
        
        # æ ¸èŒƒæ•°çš„ subgradient G = U @ Vh
        G = (U @ Vh).to(W_dense.dtype)
        print(f"  SVD æˆåŠŸ, è®¡ç®— subgradient G")

        # 3. è®¡ç®—é‡è¦æ€§åˆ†æ•°: importance
        importance = torch.abs(delta_B * G)
        #importance = delta_B * G
        #importance = -1 * importance
        print(f"  è®¡ç®— importance = |delta_B * G|, norm: {torch.norm(importance):.4f}")

        # æ‰“å°ä¸€äº›ç»Ÿè®¡ä¿¡æ¯
        current_nuclear_norm = torch.sum(S).item()
        current_rank = (S > 1e-10).sum().item()
        print(f"  Î”B çš„åˆå§‹æ ¸èŒƒæ•° = {current_nuclear_norm:.6f}, æœ‰æ•ˆç§© = {current_rank}")

    except RuntimeError as e:
        print(f"  è­¦å‘Š: SVD å¤±è´¥ ({e})ï¼Œå›é€€åˆ°äºŒé˜¶è¿‘ä¼¼ importance = |Î”B * Î”B|")
        importance = torch.abs(delta_B * delta_B)

    # 4. ä½¿ç”¨æ–°çš„ importance åˆ†æ•°å¯¹ W_dense è¿›è¡Œ 2:4 ç»“æ„åŒ–å‰ªæ
    print(f"  ä½¿ç”¨ importance å¯¹ W_dense è¿›è¡Œ 2:4 å‰ªæ...")
    W_main_optimized = prune_to_2_4_by_importance(W_dense, importance)

    return W_main_optimized


def prune_to_2_4_by_importance(matrix, importance):
    """
    åŸºäº importance çŸ©é˜µè¿›è¡Œ 2:4 ç»“æ„åŒ–å‰ªæ
    æ¯ 4 ä¸ªå…ƒç´ çš„å°ç»„é‡Œï¼Œä¿ç•™ importance æœ€å¤§çš„ 2 ä¸ªï¼Œå…¶ä»–ç½®é›¶
    ä¿®æ”¹ä¸ºWandaåŸç‰ˆå¤„ç†æ–¹å¼ï¼šå¿½ç•¥æœ«å°¾ä¸è¶³4åˆ—çš„éƒ¨åˆ†ã€‚
    """
    result = matrix.clone()
    rows, cols = matrix.shape

    prune_mask = torch.zeros_like(matrix, dtype=torch.bool)

    prune_n, prune_m = 2, 4  # æ¯4ä¸ªå…ƒç´ ä¸­ä¿ç•™2ä¸ª

    # åŸç‰ˆWandaé€»è¾‘ï¼šåªå¤„ç†å®Œæ•´çš„4åˆ—å—
    for ii in range(0, cols - cols % prune_m, prune_m):
        importance_block = importance[:, ii:ii+prune_m]
        # å®Œæ•´ 4 åˆ—å—ï¼Œå‰ªæ‰ importance æœ€å°çš„ 2 ä¸ª
        _, to_prune_indices = torch.topk(importance_block, prune_n, dim=1, largest=True)
        prune_mask.scatter_(1, ii + to_prune_indices, True)

    # ç½®é›¶æœ€ä¸é‡è¦çš„å…ƒç´ 
    result[prune_mask] = 0

    return result


# ç»˜åˆ¶å¥‡å¼‚å€¼åˆ†å¸ƒå›¾
def plot_singular_values_of_delta_B(W_unstructured, W_main_pruned, title, save_dir="plots", svd_tolerance=1e-10):
    """ç»˜åˆ¶ Î”B = W_unstructured - W_main_pruned çš„å¥‡å¼‚å€¼åˆ†å¸ƒå›¾"""
    os.makedirs(save_dir, exist_ok=True)
    
    delta_B = W_unstructured - W_main_pruned
    
    if torch.norm(delta_B) < 1e-12: # é¿å…å¯¹å‡ ä¹ä¸ºé›¶çš„çŸ©é˜µè¿›è¡ŒSVD
        print(f"  Warning: Delta B matrix ({title}) is close to zero, skipping SVD analysis")
        return
    
    with torch.no_grad():
        try:
            # å°è¯•åœ¨GPUä¸Šè¿›è¡ŒSVD
            U, S_vals, Vh = torch.linalg.svd(delta_B, full_matrices=False)
        except RuntimeError as e:
            if "out of memory" in str(e):
                print(f"  GPUå†…å­˜ä¸è¶³ï¼Œå°†SVDè®¡ç®—ç§»åˆ°CPUè¿›è¡Œ: {e}")
                try:
                    # ç§»åˆ°CPUè¿›è¡Œè®¡ç®—
                    delta_B_cpu = delta_B.cpu()
                    U, S_vals, Vh = torch.linalg.svd(delta_B_cpu, full_matrices=False)
                    S_vals = S_vals.cpu()  # ç¡®ä¿åœ¨CPUä¸Š
                except Exception as cpu_e:
                    print(f"  Error: Delta B matrix ({title}) SVD calculation failed even on CPU - {cpu_e}")
                    return
            else:
                print(f"  Error: Delta B matrix ({title}) SVD calculation failed - {e}")
                return
        
        # è¿‡æ»¤æ‰æ•°å€¼ä¸Šæ¥è¿‘é›¶çš„å¥‡å¼‚å€¼
        S_vals_filtered = S_vals[S_vals > svd_tolerance]
        
        if len(S_vals_filtered) == 0:
            print(f"  Warning: All singular values of Delta B matrix ({title}) are close to zero (below {svd_tolerance}), skipping plot")
            return
        
        plt.figure(figsize=(10, 5))
        plt.plot(S_vals_filtered.cpu().numpy(), 'o-', markersize=4)
        plt.title(f'{title} - Delta B Singular Values Distribution', fontsize=14)
        plt.xlabel('Singular Value Index', fontsize=12)
        plt.ylabel('Singular Value (Log Scale)', fontsize=12)
        plt.yscale('log')
        plt.grid(True, linestyle='--', alpha=0.6)
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        plt.text(0.7, 0.8, f'Effective Singular Values: {len(S_vals_filtered)}\n'
                          f'Max Singular Value: {S_vals_filtered[0]:.2e}\n'
                          f'Nuclear Norm: {torch.sum(S_vals_filtered):.2e}', 
                 transform=plt.gca().transAxes, 
                 bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue", alpha=0.7))
        
        filename = f"{title.replace(' ', '_').replace('-', '_').replace(':', '')}_singular_values.png"
        filepath = os.path.join(save_dir, filename)
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"  Plot saved: {filepath}")


# ç»˜åˆ¶ç´¯ç§¯èƒ½é‡åˆ†å¸ƒå›¾
def plot_cumulative_energy_of_delta_B(W_unstructured, W_main_pruned, title, save_dir="plots", svd_tolerance=1e-10):
    """
    ç»˜åˆ¶ Î”B = W_unstructured - W_main_pruned çš„ç´¯ç§¯èƒ½é‡åˆ†å¸ƒå›¾
    Xè½´ï¼šä¿ç•™çš„å¥‡å¼‚å€¼æ•°é‡ï¼ˆNï¼‰
    Yè½´ï¼šä¿ç•™çš„å‰Nä¸ªå¥‡å¼‚å€¼æ‰€ç´¯ç§¯çš„èƒ½é‡å æ€»èƒ½é‡çš„ç™¾åˆ†æ¯”
    """
    os.makedirs(save_dir, exist_ok=True)
    
    delta_B = W_unstructured - W_main_pruned
    
    if torch.norm(delta_B) < 1e-12: # é¿å…å¯¹å‡ ä¹ä¸ºé›¶çš„çŸ©é˜µè¿›è¡ŒSVD
        print(f"  Warning: Delta B matrix ({title}) is close to zero, skipping cumulative energy analysis")
        return
    
    with torch.no_grad():
        try:
            # å°è¯•åœ¨GPUä¸Šè¿›è¡ŒSVD
            U, S_vals, Vh = torch.linalg.svd(delta_B, full_matrices=False)
        except RuntimeError as e:
            if "out of memory" in str(e):
                print(f"  GPUå†…å­˜ä¸è¶³ï¼Œå°†SVDè®¡ç®—ç§»åˆ°CPUè¿›è¡Œ: {e}")
                try:
                    # ç§»åˆ°CPUè¿›è¡Œè®¡ç®—
                    delta_B_cpu = delta_B.cpu()
                    U, S_vals, Vh = torch.linalg.svd(delta_B_cpu, full_matrices=False)
                    S_vals = S_vals.cpu()  # ç¡®ä¿åœ¨CPUä¸Š
                except Exception as cpu_e:
                    print(f"  Error: Delta B matrix ({title}) SVD calculation failed even on CPU - {cpu_e}")
                    return
            else:
                print(f"  Error: Delta B matrix ({title}) SVD calculation failed - {e}")
                return
        
        # è¿‡æ»¤æ‰æ•°å€¼ä¸Šæ¥è¿‘é›¶çš„å¥‡å¼‚å€¼
        S_vals_filtered = S_vals[S_vals > svd_tolerance]
        
        if len(S_vals_filtered) == 0:
            print(f"  Warning: All singular values of Delta B matrix ({title}) are close to zero (below {svd_tolerance}), skipping cumulative energy plot")
            return
        
        # è®¡ç®—ç´¯ç§¯èƒ½é‡
        S_vals_np = S_vals_filtered.cpu().numpy()
        total_energy = np.sum(S_vals_np)  # æ€»èƒ½é‡ï¼ˆæ ¸èŒƒæ•°ï¼‰
        cumulative_energy = np.cumsum(S_vals_np)  # ç´¯ç§¯èƒ½é‡
        cumulative_energy_ratio = cumulative_energy / total_energy * 100  # ç´¯ç§¯èƒ½é‡ç™¾åˆ†æ¯”
        
        # ç»˜åˆ¶ç´¯ç§¯èƒ½é‡å›¾
        plt.figure(figsize=(12, 6))
        
        # ä¸»å›¾ï¼šç´¯ç§¯èƒ½é‡ç™¾åˆ†æ¯”
        plt.plot(range(1, len(cumulative_energy_ratio) + 1), cumulative_energy_ratio, 'b-', linewidth=2, label='Cumulative Energy %')
        plt.fill_between(range(1, len(cumulative_energy_ratio) + 1), 0, cumulative_energy_ratio, alpha=0.3, color='blue')
        
        # æ·»åŠ ä¸€äº›é‡è¦çš„æ°´å¹³çº¿
        plt.axhline(y=50, color='red', linestyle='--', alpha=0.7, label='50% Energy')
        plt.axhline(y=80, color='orange', linestyle='--', alpha=0.7, label='80% Energy')
        plt.axhline(y=90, color='green', linestyle='--', alpha=0.7, label='90% Energy')
        plt.axhline(y=95, color='purple', linestyle='--', alpha=0.7, label='95% Energy')
        
        # æ‰¾åˆ°è¾¾åˆ°å„ä¸ªèƒ½é‡é˜ˆå€¼æ‰€éœ€çš„å¥‡å¼‚å€¼æ•°é‡
        thresholds = [50, 80, 90, 95]
        threshold_positions = []
        for thresh in thresholds:
            idx = np.argmax(cumulative_energy_ratio >= thresh)
            if cumulative_energy_ratio[idx] >= thresh:
                threshold_positions.append((idx + 1, thresh))
                plt.annotate(f'{thresh}% at N={idx + 1}', 
                           xy=(idx + 1, thresh), xytext=(idx + 1 + len(S_vals_np)*0.1, thresh + 5),
                           arrowprops=dict(arrowstyle='->', color='black', alpha=0.6),
                           fontsize=10, ha='left')
        
        plt.title(f'{title} - Cumulative Energy Distribution', fontsize=14, fontweight='bold')
        plt.xlabel('Number of Singular Values Retained (N)', fontsize=12)
        plt.ylabel('Cumulative Energy Percentage (%)', fontsize=12)
        plt.grid(True, alpha=0.3)
        plt.legend()
        plt.ylim(0, 105)
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        info_text = f'Total Singular Values: {len(S_vals_filtered)}\n'
        info_text += f'Total Energy (Nuclear Norm): {total_energy:.2e}\n'
        info_text += f'Max Singular Value: {S_vals_np[0]:.2e}\n'
        
        # æ·»åŠ èƒ½é‡åˆ†å¸ƒå…³é”®ä¿¡æ¯
        for pos, thresh in threshold_positions:
            info_text += f'{thresh}% Energy: Top {pos} SVs\n'
        
        plt.text(0.02, 0.98, info_text, transform=plt.gca().transAxes, 
                fontsize=10, verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.9))
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾åƒ
        filename = f"{title.replace(' ', '_').replace('-', '_').replace(':', '')}_cumulative_energy.png"
        filepath = os.path.join(save_dir, filename)
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"  Cumulative energy plot saved: {filepath}")
        
        # æ‰“å°ä¸€äº›å…³é”®ç»Ÿè®¡ä¿¡æ¯
        print(f"    Total energy (nuclear norm): {total_energy:.6f}")
        for pos, thresh in threshold_positions:
            energy_ratio = pos / len(S_vals_filtered) * 100
            print(f"    {thresh}% energy achieved with top {pos} singular values ({energy_ratio:.1f}% of all SVs)")
        
        return filepath
        

# --- ä¸»æ‰§è¡Œé€»è¾‘ ---
if __name__ == '__main__':
    print("="*60)
    print("æ ¸èŒƒæ•°æœ€å°åŒ– + 2:4ç»“æ„åŒ–ç¨€ç–çº¦æŸä¼˜åŒ–å®éªŒ")
    print("="*60)
    
    # 1. ç”Ÿæˆéšæœºç¨ å¯†æƒé‡çŸ©é˜µå’Œæ¿€æ´»ç»Ÿè®¡
    W_dense, scaler_row, X_activation = generate_dense_matrix_and_activations(
        INPUT_DIM, OUTPUT_DIM, NUM_SAMPLES
    )
    
    # è®°å½•åŸå§‹åˆ—æ•°ï¼Œç”¨äºåç»­è¯„ä¼°
    original_cols = W_dense.shape[1]
    print(f"  ä½¿ç”¨åŸå§‹æƒé‡çŸ©é˜µï¼Œåˆ—æ•°: {original_cols} (å¯èƒ½ä¸æ˜¯4çš„å€æ•°)")
    
    # 2. åº”ç”¨Wanda 50%éç»“æ„åŒ–å‰ªæå¾—åˆ°W_unstructured
    W_unstructured = apply_wanda_unstructured_pruning(W_dense, scaler_row, sparsity_ratio=0.5)
    
    # 3. ç”±W_denseç›´æ¥2:4ç»“æ„åŒ–å‰ªæå¾—åˆ°W_structured (Wanda baseline)
    print("\n" + "="*50)
    print("æ­¥éª¤3: ç”±ç¨ å¯†çŸ©é˜µç›´æ¥2:4ç»“æ„åŒ–å‰ªæ (Wanda baseline)")
    print("="*50)
    W_structured = prune_2_4_baseline(W_dense, scaler_row)
    
    # 4. ç”¨æ ¸èŒƒæ•°importance-basedæ–¹æ³•ä¼˜åŒ–2:4å‰ªæ
    print("\n" + "="*50)
    print("æ­¥éª¤4: æ ¸èŒƒæ•°importance-based 2:4å‰ªæä¼˜åŒ–")
    print("="*50)
    print("  è¯¥æ–¹æ³•æ—¨åœ¨æ‰¾åˆ°ä¸€ä¸ª2:4ç¨€ç–çŸ©é˜µï¼Œä½¿ä¹‹ä¸W_unstructuredçš„å·®å€¼æ ¸èŒƒæ•°æœ€å°åŒ–")
    W_nuclear_optimized = prune_2_4_with_nuclear_norm_importance(W_unstructured, W_dense)
    
    # éªŒè¯æ‰€æœ‰çŸ©é˜µçš„2:4ç¨€ç–æ€§
    print("\n" + "="*50)
    print("ç¨€ç–æ€§éªŒè¯")
    print("="*50)
    matrices = {
        'W_structured (Wanda baseline)': W_structured,
        'W_nuclear_optimized (æ ¸èŒƒæ•°ä¼˜åŒ–)': W_nuclear_optimized
    }
    
    for name, matrix in matrices.items():
        is_2_4 = check_2_4_sparsity(matrix)
        sparsity = calculate_sparsity(matrix)
        print(f"  {name}: 2:4æ¨¡å¼={is_2_4}, ç¨€ç–ç‡={sparsity:.1%}")
    
    # 5. ç»¼åˆè¯„ä¼°å’Œæ¯”è¾ƒ
    print("\n" + "="*60)
    print("æ­¥éª¤5: ç»¼åˆè¯„ä¼°å’Œæ¯”è¾ƒç»“æœ")
    print("="*60)
    
    # 5a. æ ¸èŒƒæ•°å’Œç§©çš„æ¯”è¾ƒ - ä»¥W_unstructuredä¸ºå‚è€ƒ
    print("\n--- A. ä¸¤ç§2:4å‰ªææ–¹æ³•çš„Î”Båˆ†æ (ç›¸å¯¹äºW_unstructured) ---")
    
    # è®¡ç®—å„çŸ©é˜µç›¸å¯¹äºW_unstructuredçš„Î”BæŒ‡æ ‡
    rank_structured, nuc_norm_structured = calculate_delta_metrics(W_unstructured, W_structured)
    rank_nuclear_optimized, nuc_norm_nuclear_optimized = calculate_delta_metrics(W_unstructured, W_nuclear_optimized)
    
    print(f"  W_structured (Wanda baseline) Î”B: æœ‰æ•ˆç§©={rank_structured:4d}, Nuclear Norm={nuc_norm_structured:10.6f}")
    print(f"  W_nuclear_optimized (æ ¸èŒƒæ•°ä¼˜åŒ–) Î”B: æœ‰æ•ˆç§©={rank_nuclear_optimized:4d}, Nuclear Norm={nuc_norm_nuclear_optimized:10.6f}")
    
    # æ¯”è¾ƒæ”¹è¿›æ•ˆæœ
    print(f"\n  æ ¸èŒƒæ•°ä¼˜åŒ–ç›¸å¯¹äºWanda baselineçš„æ”¹è¿›æ•ˆæœ:")
    rank_improvement = rank_structured - rank_nuclear_optimized
    nuc_norm_improvement = nuc_norm_structured - nuc_norm_nuclear_optimized
    print(f"    ç§©é™ä½: {rank_improvement} ({rank_improvement/max(rank_structured,1)*100:.1f}%)")
    print(f"    æ ¸èŒƒæ•°é™ä½: {nuc_norm_improvement:.6f} ({nuc_norm_improvement/max(nuc_norm_structured,1e-8)*100:.1f}%)")
    
    # 5b. åŠŸèƒ½æŸå¤±æ¯”è¾ƒ
    print("\n--- B. åŠŸèƒ½æŸå¤±æ¯”è¾ƒ (ç›¸å¯¹äºW_unstructured) ---")
    print("  (è®¡ç®— ||W_unstructured @ X - W_method @ X||_F)")
    
    # ä½¿ç”¨æ¿€æ´»æ•°æ®è¿›è¡Œè¯„ä¼°
    X_eval = X_activation
    W_unstructured_eval = W_unstructured
    W_structured_eval = W_structured
    W_nuclear_optimized_eval = W_nuclear_optimized
    
    error_structured = functional_loss(W_unstructured_eval, W_structured_eval, X_eval)
    error_nuclear_optimized = functional_loss(W_unstructured_eval, W_nuclear_optimized_eval, X_eval)
    
    print(f"  W_structured (Wanda baseline)åŠŸèƒ½æŸå¤±: {error_structured:.6f}")
    print(f"  W_nuclear_optimized (æ ¸èŒƒæ•°ä¼˜åŒ–)åŠŸèƒ½æŸå¤±: {error_nuclear_optimized:.6f}")
    
    print(f"\n  åŠŸèƒ½æŸå¤±æ”¹è¿›:")
    error_improvement = error_structured - error_nuclear_optimized
    print(f"    æ ¸èŒƒæ•°ä¼˜åŒ– vs Wanda baseline: {error_improvement:.6f} ({'æ”¹å–„' if error_improvement > 0 else 'ç•¥æœ‰å¢åŠ ' if error_improvement < 0 else 'ä¿æŒä¸å˜'})")




    # === æ–°å¢ï¼šä½ç§©è¡¥å¿åŠŸèƒ½æŸå¤±è¯„ä¼° ===
    print("\n--- C. ä½ç§©è¡¥å¿åŠŸèƒ½æŸå¤±è¯„ä¼° ---")
    # è°ƒæ•´ä¿®æ”¹ä½ç§©è¿‘ä¼¼çš„ç§©k
    k = 64
    print(f"  ä½ç§©è¿‘ä¼¼ä¿ç•™å‰ {k} ä¸ªå¥‡å¼‚å€¼")

    # è®¡ç®—Î”B
    deltaB_structured = W_unstructured - W_structured
    deltaB_nuclear_optimized = W_unstructured - W_nuclear_optimized

    print(f"  æ­£åœ¨è®¡ç®— Î”B_structured çš„ä½ç§©è¿‘ä¼¼...")
    # ä½ç§©è¿‘ä¼¼
    deltaB_structured_LRA = low_rank_approximation(deltaB_structured, k)
    print(f"  æ­£åœ¨è®¡ç®— Î”B_nuclear_optimized çš„ä½ç§©è¿‘ä¼¼...")
    deltaB_nuclear_optimized_LRA = low_rank_approximation(deltaB_nuclear_optimized, k)

    # æ„é€ è¡¥å¿åæƒé‡
    W_compensated_structured = W_structured + deltaB_structured_LRA
    W_compensated_nuclear_optimized = W_nuclear_optimized + deltaB_nuclear_optimized_LRA

    # è®¡ç®—è¡¥å¿ååŠŸèƒ½æŸå¤±
    error_compensated_structured = functional_loss(W_unstructured, W_compensated_structured, X_eval)
    error_compensated_nuclear_optimized = functional_loss(W_unstructured, W_compensated_nuclear_optimized, X_eval)

    print(f"  W_structured + Î”B_structured_LRA åŠŸèƒ½æŸå¤±: {error_compensated_structured:.6f}")
    print(f"  W_nuclear_optimized + Î”B_nuclear_optimized_LRA åŠŸèƒ½æŸå¤±: {error_compensated_nuclear_optimized:.6f}")
    
    # æ¯”è¾ƒä½ç§©è¡¥å¿åçš„æ”¹è¿›æ•ˆæœ
    print(f"\n  ä½ç§©è¡¥å¿åçš„æ”¹è¿›æ•ˆæœ:")
    compensated_improvement = error_compensated_structured - error_compensated_nuclear_optimized
    print(f"    æ ¸èŒƒæ•°ä¼˜åŒ– vs Wanda baseline (è¡¥å¿å): {compensated_improvement:.6f} ({'æ”¹å–„' if compensated_improvement > 0 else 'ç•¥æœ‰å¢åŠ ' if compensated_improvement < 0 else 'ä¿æŒä¸å˜'})")

    # 5c. ç¨€ç–åº¦æ€»ç»“
    print("\n--- D. ç¨€ç–åº¦æ€»ç»“ ---")
    sparsity_dense = calculate_sparsity(W_dense)
    sparsity_unstructured = calculate_sparsity(W_unstructured)
    sparsity_structured = calculate_sparsity(W_structured)
    sparsity_nuclear_optimized = calculate_sparsity(W_nuclear_optimized)
    
    print(f"  W_dense (åŸå§‹ç¨ å¯†): {sparsity_dense:.2%}")
    print(f"  W_unstructured (Wanda 50%éç»“æ„åŒ–): {sparsity_unstructured:.2%}")
    print(f"  W_structured (Wanda baseline 2:4): {sparsity_structured:.2%}")
    print(f"  W_nuclear_optimized (æ ¸èŒƒæ•°ä¼˜åŒ–2:4): {sparsity_nuclear_optimized:.2%}")
    print(f"  2:4ç†è®ºç¨€ç–åº¦: 50.00%")
    
    # 6. å¯è§†åŒ–ç»“æœ
    print("\n--- E. Delta B Analysis Plots ---")
    print("  ç”Ÿæˆå¥‡å¼‚å€¼åˆ†å¸ƒå›¾...")
    plot_singular_values_of_delta_B(W_unstructured, W_structured, "W_structured_vs_W_unstructured")
    plot_singular_values_of_delta_B(W_unstructured, W_nuclear_optimized, "W_nuclear_optimized_vs_W_unstructured")
    
    print("  ç”Ÿæˆç´¯ç§¯èƒ½é‡åˆ†å¸ƒå›¾...")
    plot_cumulative_energy_of_delta_B(W_unstructured, W_structured, "W_structured_vs_W_unstructured")
    plot_cumulative_energy_of_delta_B(W_unstructured, W_nuclear_optimized, "W_nuclear_optimized_vs_W_unstructured")
    
    # 7. ç»“è®º
    print("\n" + "="*60)
    print("æ­¥éª¤6: å®éªŒç»“è®º")
    print("="*60)
    
    if rank_nuclear_optimized < rank_structured and nuc_norm_nuclear_optimized < nuc_norm_structured:
        print("âœ… æˆåŠŸ! æ ¸èŒƒæ•°ä¼˜åŒ–æ–¹æ³•åœ¨é™ä½Î”Bçš„ç§©å’Œæ ¸èŒƒæ•°æ–¹é¢éƒ½ä¼˜äºWanda baseline")
        print(f"   ç§©ä» {rank_structured} é™ä½åˆ° {rank_nuclear_optimized}")
        print(f"   æ ¸èŒƒæ•°ä» {nuc_norm_structured:.6f} é™ä½åˆ° {nuc_norm_nuclear_optimized:.6f}")
    elif rank_nuclear_optimized < rank_structured:
        print("âœ… éƒ¨åˆ†æˆåŠŸ! æ ¸èŒƒæ•°ä¼˜åŒ–æ–¹æ³•æˆåŠŸé™ä½äº†Î”Bçš„ç§©")
        print(f"   ç§©ä» {rank_structured} é™ä½åˆ° {rank_nuclear_optimized}")
        print(f"   ä½†æ ¸èŒƒæ•°å˜åŒ–: {nuc_norm_structured:.6f} -> {nuc_norm_nuclear_optimized:.6f}")
    elif nuc_norm_nuclear_optimized < nuc_norm_structured:
        print("âœ… éƒ¨åˆ†æˆåŠŸ! æ ¸èŒƒæ•°ä¼˜åŒ–æ–¹æ³•æˆåŠŸé™ä½äº†Î”Bçš„æ ¸èŒƒæ•°")
        print(f"   æ ¸èŒƒæ•°ä» {nuc_norm_structured:.6f} é™ä½åˆ° {nuc_norm_nuclear_optimized:.6f}")
        print(f"   ä½†ç§©å˜åŒ–: {rank_structured} -> {rank_nuclear_optimized}")
    else:
        print("âŒ å®éªŒæœªè¾¾åˆ°é¢„æœŸæ•ˆæœï¼Œéœ€è¦è°ƒæ•´å‚æ•°æˆ–æ–¹æ³•")
    
    print(f"\nåŠŸèƒ½æŸå¤±æ–¹é¢: {'æ”¹å–„' if error_improvement > 0 else 'ç•¥æœ‰å¢åŠ ' if error_improvement < 0 else 'ä¿æŒä¸å˜'}")
    
    # 8. æ ¸å¿ƒå‘ç°æ€»ç»“
    print(f"\næ­¥éª¤7: æ ¸å¿ƒå‘ç°æ€»ç»“")
    print(f"ğŸ¯ æ ¸å¿ƒå‘ç°:")
    if nuc_norm_nuclear_optimized < nuc_norm_structured:
        print("   - æ ¸èŒƒæ•°importance-basedæ–¹æ³•æœ‰æ•ˆé™ä½äº†Î”Bçš„æ ¸èŒƒæ•°ï¼Œè¯æ˜äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§")
    if rank_nuclear_optimized < rank_structured:
        print("   - ä¼˜åŒ–æ–¹æ³•å®ç°äº†æ›´å¥½çš„ä½ç§©è¿‘ä¼¼")
    if error_improvement > 0:
        print("   - åœ¨é™ä½æ ¸èŒƒæ•°çš„åŒæ—¶ä¿æŒäº†æ›´å¥½çš„åŠŸèƒ½è¿‘ä¼¼è´¨é‡")
    elif error_improvement < 0:
        print("   - æ ¸èŒƒæ•°é™ä½ä½†åŠŸèƒ½æŸå¤±ç•¥æœ‰å¢åŠ ï¼Œè¿™æ˜¯ä½ç§©è¿‘ä¼¼çš„å¸¸è§trade-off")
    
    print("\n--- å®éªŒå®Œæˆ ---")